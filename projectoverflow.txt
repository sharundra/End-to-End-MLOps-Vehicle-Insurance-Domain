1. Create a repo "End-to-End-MLOps-Vehicle-Insurance-Domain" and clone it in local
2. Create a script template.py and run it which upon running would generate project folder template.
3. write the scripts of setup.py and pyproject.toml to import local changes.
4. Open/create a conda venv and install requirements.txt -- "pip install -r requirements.txt"
5. Run a "pip list" in terminal to confirm that their are no "src" pacakge yet.
6. To use src (or the contents inside it) as a package, add "-e ." at the of requirements.txt and rerun "pip install -r requirements.txt".
    a. Again run "pip list" to ensure "src" is installed as a package.
    # "-e ." goes to setup.py and pyproject.toml to achieve this goal.
    # refer bonus_notes.txt to understand more.

----------------------------------MongoDB Setup--------------------------------------
1. Sign up to MongoDB atlas and while signing in, fill company name so that you are an organization on MongoDB.
# At MongoDB, we have Organization, which lets us create various projects and inside projects, we create clusters.
2. Click on "Sharad's Organization" and inside that "create a porject" choosing default options like no tags and permissions.
3. Inside project, click on "create a cluster", choose free cluster(512 MB storage and shared RAM), keep other things default like "provider" as "aws".
4. Now, copy the generated username and password for this cluster.
5. Add an IP address to access it from anywhere -- 
    a. In the left panel, go to Security --> Databasse & Network Access --> Network access --> IP access list --> Add IP address --> put "0.0.0.0/0" (this will make cluster to be accessed from anywhere).
6. Go back to project and click on "get connection string" --> for "driver", select "python" and for "version", select "4.7 or later".
    a. Then copy the connection string and store it somewhere, It will be used later. 
    # Earlier copied password will be required to put in connection string to use that.
7. Create a folder "notebook" and add dataset "data.csv"
8. Create a notebook file "mongodb_demo.ipynb" inside "notebook" folder and select currently using conda venv as kernel there.
9. Run the script and hence use the connection string in the "mongodb_demo.ipynb" to push the data (data.csv) to our project's cluter in mongodb.
    # After working with the data, either delete the cluster from mongodb or remove the connection string/password from the script.
10. Check Mongodb atlas to confirm that data is pushed properly -- project --> clusters --> browse collections.

-------------------------------------- logging, exception and notebooks --------------------------------------
1. Create the logging script in src/logger/__init__.py
2. To check if its working properly, run demo.py (only logging part). It should create a folder logs with a log file inside it.
3. Similarily, create the exception script too in src/exception/__init__.pywhich logs the error/exception. Test it in demo.py and check the log folder. 


-------------------------------------- Preprocessing experiments in notebook --------------------------------------
1. Add script exp-notebook.ipynb in the "notebook" folder to perform EDA on the dataset.

----------------------------------------------- Data Ingestion ----------------------------------------------------
Adding constants:
1. Add constant variables in src/constants.__init__.py

Accessing MongoDB database through configuration and data_access:
2. Add script src/configuration/mongo_db_connection.py to connect to mongodb database.
3. Add script src/data_access/proj1_data.py to import data from MongoDB as a pandas dataframe using the connection establishement script of src/configuration/mongo_db_connection.py.

Doing data ingestion through entity and components:
4. Add path related configurations of Data Ingestion into src/entity/config_entity.py.
    a. Add DataIngestionArtifact class to src/entity/artifact_entity.py which would have artifact storing paths (train_file_path and test_file_path). 
    b. Add script to src/components/data_ingestion.py which would access the dataframe ans store it at feature_store path. Also, it will do train_test_split of the data and store them separately at train_file_path and test_file_path.

Setting and testing training pipeline:
5. Inside src/pipeline/training_pipeline.py, setup only data ingestion part.
6. Setup MONGODB_URL_KEY in src/configuration/mongo_db_connection.py
7. Run demo.py just to run data ingestion pipeline.
 # Add artifact folder to .gitignote

 ---------------------------- Data Validation, Data Transformation & Model Trainer ----------------------------
 1. Prepare the script of src/utils/main_utils.py and config/schema.yaml
 2. Now add scripts pretty much the same way it was done for data ingestion like adding constants, config_entity, artifact_entity, components, pipeline.
 3. Run demo.py to confirm both data ingestion and data validation are working fine.
 4. Repeat step 2 and 3 for Data Transformation. 
 5. Repeat step 2 and 3 for Model Trainer (Add src/entity/estimator.py also which would primarily be used in Model Evaluation but here this class is intialized with trained model and saved for later use).

------------------------------------------ AWS Services Setup --------------------------------------------------
1. Login to AWS consol (prefer via root user) and keep region as "us-east-1".
2. Create a new user (name -- firstproj) in IAM. GO to IAM --> "users" (in the left sidebar) --> "create user" : enter the name of user, ie.. firstproj, then click next (DONT select "user access to management consol") --> "attach policies directly" 
 # For the policy, select "AdministratorAccess" (it means user has access to all services), click next --> "create user"
3. Create access key (for authentication whenever AWS services will be used): "users" --> "security credentials" --> "access keys" --> "create access key" --> "CLI" --> "confirmation" --> "create access key".
 # Copy the access key and secret password. We can download the .csv file also.
4. Set AWS credentials as env variables in terminal:
    # for bash or zsh terminal -- export AWS_ACCESS_KEY_ID="AWS_ACCESS_KEY_ID" export AWS_SECRET_ACCESS_KEY="AWS_SECRET_ACCESS_KEY"
    # for powershell -- $env:AWS_ACCESS_KEY_ID="AWS_ACCESS_KEY_ID" $env:AWS_SECRET_ACCESS_KEY="AWS_SECRET_ACCESS_KEY"
    a. Check if they are set properly -- 
    # for bash/zsh -- echo $AWS_ACCESS_KEY_ID      and       echo $AWS_SECRET_ACCESS_KEY
    # for powershell -- echo $env:AWS_ACCESS_KEY_ID      and       echo $env:AWS_SECRET_ACCESS_KEY
5. Add these AWS keys (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) and REGION_NAME = "us-east-1" in src.constants/__init__.py
6. Add script to src/configuration/aws_connection.py to establish connection with S3 bucket using AWS credentials. 
    # It uses boto3 library which is AWS' official SDK for python to programmatically interact wit AWS services. 
7. Ensure that the below keys are set in constants/__init__.py :
            MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE: float = 0.02
            MODEL_BUCKET_NAME = "<unique-bucket-name>"
            MODEL_PUSHER_S3_KEY = "model-registry"
8. Create S3 bucket on AWS: search S3 --> "create bucket" --> ensure region is "us-eat-1", select "General purpose", give a unique bucket name <unique-bucket-name>, uncheck the box which says "block all public access" (for sensitive work this would remain checked), acknowledge your unchecking then go down and click on "create bucket".
9. Now add code inside src/cloud_storage/aws_storage.py. 
 # This script provides methods to interact with AWS S3 storage, file management, data uploads, and data retrieval in S3 buckets.
10. Add code to src/entity/s3_estimator.py
    # It uses functions from aws_connection.py and aws_storage.py to save and retrieve model from S3 bucket and do prediction.


------------------------------------------ Model Evaluation and Model Pusher --------------------------------------------------
 1. Add scripts for evaluation and pushing the model (to AWS) in the similar fashion as done previously i.e. constants, entity, components and pipeline
 2. Run demo.py
 # Check S3 bucket if model is pushed there or not (happens only when new trained model is better than already pushed model).

 ------------------------------------------ Making an Application --------------------------------------------------
 1. Create script for src/pipeline/prediction_pipeline.py
 2. Add VehiclePredictorConfig in src/entity/config_entity.py
 3. Create script for /app.py
 4. Create scripts for static/css/style.css and templates/vehicledata.html
 5. Run app.py and check the app at localhost:5000
 # We can train the model also through app by just clicking "train the model" button on the app.

 ---------------------------------------------------- CI/CD --------------------------------------------------------
 1. Setup the Dockerfile and .dockerignore file.
 2. Create a directory .github/workflows and add aws.yaml file inside it.
 # Through githu actions, we perform CI/CD. aws.yaml defines steps to do it.
 3. If already not created a AWS console IAM user, then create one. Since we have already created, we would refer those credentials now.
 # Refer above steps to create a new IAM user if not created already.
 4. Create a repository in ECR: Go to ECR --> "create repository" --> give a name to repo (lets say vehicleproj) and then click on "create"
 # Copy the repo name and URI and save it for later use.
 5. Now we will create a Ubuntu server on EC2. 
 # Unlike the previous project, where Github Actions' server was performing the CI, here we would be doing CD on our self hosted server (as mentioned in aws.yaml) created on EC2.
    a. AWS console >> EC2 >> Launch Instance >> name: vehicledata-machine
        >> Image: Ubuntu >> AMI: Ubuntu Server 24.04 (free tier) >> Instance: T2 Medium (~chargeable-3.5rs/hr)
        >> create new key pair (name: proj1key) >> allow for https and http traffic >> storage: 30gb >> Launch
        >> Go to instance >> click on "Connect" >> Connect using EC2 Instance Connect 
        >> Connect (Terminal will be launched) 
6. In the newly opened EC2, Install docker in EC2 Machine:
      ## Optinal commands to run
      sudo apt-get update -y
      sudo apt-get upgrade
      ## Required (Because Docker is'nt there in our EC2 server - [docker --version])
      curl -fsSL https://get.docker.com -o get-docker.sh
      sudo sh get-docker.sh
      sudo usermod -aG docker ubuntu
      newgrp docker
7. Next step is to connect Github with EC2(Self hosted runner):
      * select your project on Github >> go to settings >> Actions >> Runner >> New self hosted runner
        >> Select OS (Linux) >> Now step by step run all "Download" related commands on EC2 server 
        >> run first "Configure" command (hit enter instead of setting a "runner group", then runner name: self-hosted)
        >> For "enter any additional label", hit enter to skip >> For "name of work folder", again hit enter
        >> Now run second "Configure" command (./run.sh) and runner will get connected to Github
        >> To crosscheck, go back to Github projetc and click on settings/actions/Runner and you will see runner state as "idle"
        >> If you do ctrl+c on EC2 server then runner will shut then restart with "./run.sh"
8. Setup your Github secrets: (Github project>Settings>SecretandVariable>Actions>NewRepoSecret)
      AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_DEFAULT_REGION
      ECR_REPO
    # If we want to add the option of model training too, we woul have to give mongodb url too as secret key.
9. CI-CD pipeline will be triggered at next commit and push so add, commit and push the updates to github.
10. Now we need to activate the 5000 port of our EC2 instance:
      * Go to the instance > Security > Go to Security Groups > Edit inbound rules > add rule
        > type: Custom TCP > Port range: 5000 > 0.0.0.0/0 > Save rules
11. Now after a minute, go back to EC2 --> instances(running) --> click on instance id --> copy 'public IPv4 address'
12. Now paste the public ip address on the address bar +:5000 and our app will be launched.

