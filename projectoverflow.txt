1. Create a repo "End-to-End-MLOps-Vehicle-Insurance-Domain" and clone it in local
2. Create a script template.py and run it which upon running would generate project folder template.
3. write the scripts of setup.py and pyproject.toml to import local changes.
4. Open/create a conda venv and install requirements.txt -- "pip install -r requirements.txt"
5. Run a "pip list" in terminal to confirm that their are no "src" pacakge yet.
6. To use src (or the contents inside it) as a package, add "-e ." at the of requirements.txt and rerun "pip install -r requirements.txt".
    a. Again run "pip list" to ensure "src" is installed as a package.
    # "-e ." goes to setup.py and pyproject.toml to achieve this goal.
    # refer bonus_notes.txt to understand more.

----------------------------------MongoDB Setup--------------------------------------
1. Sign up to MongoDB atlas and while signing in, fill company name so that you are an organization on MongoDB.
# At MongoDB, we have Organization, which lets us create various projects and inside projects, we create clusters.
2. Click on "Sharad's Organization" and inside that "create a porject" choosing default options like no tags and permissions.
3. Inside project, click on "create a cluster", choose free cluster(512 MB storage and shared RAM), keep other things default like "provider" as "aws".
4. Now, copy the generated username and password for this cluster.
5. Add an IP address to access it from anywhere -- 
    a. In the left panel, go to Security --> Databasse & Network Access --> Network access --> IP access list --> Add IP address --> put "0.0.0.0/0" (this will make cluster to be accessed from anywhere).
6. Go back to project and click on "get connection string" --> for "driver", select "python" and for "version", select "4.7 or later".
    a. Then copy the connection string and store it somewhere, It will be used later. 
    # Earlier copied password will be required to put in connection string to use that.
7. Create a folder "notebook" and add dataset "data.csv"
8. Create a notebook file "mongodb_demo.ipynb" inside "notebook" folder and select currently using conda venv as kernel there.
9. Run the script and hence use the connection string in the "mongodb_demo.ipynb" to push the data (data.csv) to our project's cluter in mongodb.
    # After working with the data, either delete the cluster from mongodb or remove the connection string/password from the script.
10. Check Mongodb atlas to confirm that data is pushed properly -- project --> clusters --> browse collections.

-------------------------------------- logging, exception and notebooks --------------------------------------
1. Create the logging script in src/logger/__init__.py
2. To check if its working properly, run demo.py (only logging part). It should create a folder logs with a log file inside it.
3. Similarily, create the exception script too in src/exception/__init__.pywhich logs the error/exception. Test it in demo.py and check the log folder. 


-------------------------------------- Preprocessing experiments in notebook --------------------------------------
1. Add script exp-notebook.ipynb in the "notebook" folder to perform EDA on the dataset.

----------------------------------------------- Data Ingestion ----------------------------------------------------
Adding constants:
1. Add constant variables in src/constants.__init__.py

Accessing MongoDB database through configuration and data_access:
2. Add script src/configuration/mongo_db_connection.py to connect to mongodb database.
3. Add script src/data_access/proj1_data.py to import data from MongoDB as a pandas dataframe using the connection establishement script of src/configuration/mongo_db_connection.py.

Doing data ingestion through entity and components:
4. Add path related configurations of Data Ingestion into src/entity/config_entity.py.
    a. Add DataIngestionArtifact class to src/entity/artifact_entity.py which would have artifact storing paths (train_file_path and test_file_path). 
    b. Add script to src/components/data_ingestion.py which would access the dataframe ans store it at feature_store path. Also, it will do train_test_split of the data and store them separately at train_file_path and test_file_path.

Setting and testing training pipeline:
5. Inside src/pipeline/training_pipeline.py, setup only data ingestion part.
6. Setup MONGODB_URL_KEY in src/configuration/mongo_db_connection.py
7. Run demo.py just to run data ingestion pipeline.
 # Add artifact folder to .gitignote
